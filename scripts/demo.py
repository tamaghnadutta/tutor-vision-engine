#!/usr/bin/env python3
"""
Demo script for error detection API - showcasing all three approaches
Usage: python scripts/demo.py [--approach ocr_llm|vlm_direct|hybrid|all]
"""

import asyncio
import sys
import argparse
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import structlog
from scripts.create_dataset import create_real_dataset
from src.models.error_detector import ErrorDetector
from src.utils.logging import setup_logging

logger = structlog.get_logger()


async def run_single_approach_demo(approach: str, samples: list):
    """Demo using a specific approach"""
    print(f"\n{'='*80}")
    print(f"üî¨ TESTING: {approach.upper().replace('_', '‚Üí')} APPROACH")
    print(f"{'='*80}")

    # Map approach to description
    descriptions = {
        'ocr_llm': 'GPT-4V extracts text ‚Üí GPT-4o/Gemini analyzes for errors',
        'vlm_direct': 'GPT-4V or Gemini-2.5-Flash analyzes images directly',
        'hybrid': 'Runs both OCR‚ÜíLLM and Direct VLM, ensembles results'
    }
    print(f"üìã Strategy: {descriptions.get(approach, 'Unknown approach')}")

    # Initialize error detector with specific approach
    detector = ErrorDetector(approach=approach)

    # Test on first 2 samples for demo (to keep output manageable)
    for i, sample in enumerate(samples[:2], 1):
        await analyze_sample(detector, sample, i, approach)


async def analyze_sample(detector: ErrorDetector, sample, sample_num: int, approach: str):
    """Analyze a single sample with the given detector"""
    print(f"\n{'-'*50}")
    print(f"üìù SAMPLE {sample_num}: {sample.id}")
    print(f"üìö Topic: {sample.metadata.get('topic', 'Unknown').title()}")
    print(f"{'-'*50}")

    # Show ground truth
    if sample.ground_truth_error:
        print(f"üéØ EXPECTED: ‚ùå Error - {sample.ground_truth_error}")
    else:
        print(f"üéØ EXPECTED: ‚úÖ No errors")

    try:
        # Prepare URLs
        question_path = sample.question_url.replace("file:///", "")
        solution_path = sample.solution_url.replace("file:///", "")
        question_url = f"file://{Path.cwd()}/{question_path}"
        solution_url = f"file://{Path.cwd()}/{solution_path}"

        # Run error detection
        result = await detector.detect_errors(
            question_url=question_url,
            solution_url=solution_url,
            context={
                'sample_id': sample.id,
                'approach': approach,
                'demo_mode': True
            }
        )

        # Display results
        print(f"\nü§ñ {approach.upper().replace('_', '‚Üí')} ANALYSIS:")
        if result.get('error'):
            print(f"   üî¥ ERROR: {result['error']}")
            print(f"   üí° CORRECTION: {result.get('correction', 'N/A')}")
            print(f"   üîß HINT: {result.get('hint', 'N/A')}")
        else:
            print(f"   ‚úÖ NO ERRORS DETECTED")

        print(f"   ‚è±Ô∏è  Time: {result.get('processing_time', 0):.2f}s")
        print(f"   üéØ Confidence: {result.get('confidence', 0):.2%}")
        print(f"   üîÑ Processor: {result.get('processing_approach', 'Unknown')}")

        # Accuracy check
        gt_has_error = bool(sample.ground_truth_error)
        api_detected_error = bool(result.get('error'))

        if gt_has_error == api_detected_error:
            print(f"   ‚úÖ CORRECT PREDICTION")
        else:
            print(f"   ‚ùå INCORRECT PREDICTION ({'False Negative' if gt_has_error else 'False Positive'})")

    except Exception as e:
        print(f"   ‚ùå ERROR: {str(e)}")


async def run_comparative_demo(samples: list):
    """Run comparative demo showing all three approaches"""
    print("="*90)
    print("üî¨ COMPARATIVE DEMO - ALL THREE APPROACHES")
    print("="*90)

    approaches = ['ocr_llm', 'vlm_direct', 'hybrid']
    sample = samples[0]  # Use first sample for comparison

    print(f"\nüìù Testing Sample: {sample.id}")
    print(f"üìö Topic: {sample.metadata.get('topic', 'Unknown').title()}")
    print(f"üéØ Ground Truth: {'‚ùå Error Expected' if sample.ground_truth_error else '‚úÖ No Error Expected'}")

    results = {}

    for approach in approaches:
        print(f"\n{'-'*60}")
        print(f"Testing {approach.upper().replace('_', '‚Üí')} approach...")

        try:
            detector = ErrorDetector(approach=approach)

            question_path = sample.question_url.replace("file:///", "")
            solution_path = sample.solution_url.replace("file:///", "")
            question_url = f"file://{Path.cwd()}/{question_path}"
            solution_url = f"file://{Path.cwd()}/{solution_path}"

            result = await detector.detect_errors(
                question_url=question_url,
                solution_url=solution_url,
                context={'approach': approach, 'comparison_mode': True}
            )

            results[approach] = result

            print(f"   Result: {'‚ùå Error' if result.get('error') else '‚úÖ No Error'}")
            print(f"   Time: {result.get('processing_time', 0):.2f}s")
            print(f"   Confidence: {result.get('confidence', 0):.2%}")

        except Exception as e:
            print(f"   ‚ùå Failed: {str(e)}")
            results[approach] = {'error': None, 'processing_time': 0, 'confidence': 0}

    # Comparison table
    print(f"\n{'='*90}")
    print("üìä COMPARATIVE RESULTS")
    print(f"{'='*90}")

    print(f"{'Approach':<15} {'Result':<12} {'Time':<8} {'Confidence':<12} {'Status'}")
    print(f"{'-'*60}")

    gt_has_error = bool(sample.ground_truth_error)

    for approach in approaches:
        result = results[approach]
        has_error = bool(result.get('error'))
        time_taken = result.get('processing_time', 0)
        confidence = result.get('confidence', 0)

        status = "‚úÖ Correct" if has_error == gt_has_error else "‚ùå Wrong"
        error_text = "Error" if has_error else "No Error"

        print(f"{approach.replace('_', '‚Üí'):<15} {error_text:<12} {time_taken:.2f}s    {confidence:.1%}        {status}")


async def run_demo(approach: str = "all"):
    """Main demo function"""
    print("="*90)
    print("ERROR DETECTION API - COMPREHENSIVE DEMO")
    print("="*90)

    setup_logging()

    # Create dataset from real images
    dataset = create_real_dataset()
    samples = dataset.samples

    print(f"\nüéØ Available Samples: {len(samples)} REAL student solutions")
    print(f"   Topics: Probability, Trigonometry, Algebra, Complex Numbers")
    print(f"   Format: Real handwritten mathematical solutions")

    if approach == "all":
        await run_comparative_demo(samples)
        print(f"\n{'='*90}")
        print("üîç INDIVIDUAL APPROACH DEMOS")
        print(f"{'='*90}")
        for single_approach in ['ocr_llm', 'vlm_direct', 'hybrid']:
            await run_single_approach_demo(single_approach, samples)
    else:
        await run_single_approach_demo(approach, samples)

    # Summary
    print(f"\n{'='*90}")
    print("üìã DEMO SUMMARY")
    print(f"{'='*90}")

    if approach == "all":
        print(f"‚úÖ Demonstrated all three approaches as per assignment:")
        print(f"   ‚Ä¢ OCR‚ÜíLLM: GPT-4V OCR ‚Üí GPT-4o/Gemini reasoning")
        print(f"   ‚Ä¢ Direct VLM: GPT-4V or Gemini-2.5-Flash single call")
        print(f"   ‚Ä¢ Hybrid: Ensemble of both approaches")
    else:
        approach_names = {
            'ocr_llm': 'OCR‚ÜíLLM',
            'vlm_direct': 'Direct VLM',
            'hybrid': 'Hybrid'
        }
        print(f"‚úÖ Demonstrated {approach_names.get(approach, approach)} approach")

    print(f"\nüéØ Key Features Showcased:")
    print(f"   ‚Ä¢ Assignment-compliant approach implementations")
    print(f"   ‚Ä¢ Baseline vs improvement comparison")
    print(f"   ‚Ä¢ Real handwritten mathematical solutions")
    print(f"   ‚Ä¢ Step-level error detection with corrections")
    print(f"   ‚Ä¢ Educational feedback and hints")

    print(f"\nüöÄ Next Steps:")
    print(f"   1. Configure API keys in .env file")
    print(f"   2. Run evaluation: make eval")
    print(f"   3. Start API server: make dev")
    print(f"   4. Test specific approach: ERROR_DETECTION_APPROACH=ocr_llm make demo")


def main():
    """Main entry point with argument parsing"""
    parser = argparse.ArgumentParser(description="Demo error detection API approaches")
    parser.add_argument(
        "--approach",
        choices=['ocr_llm', 'vlm_direct', 'hybrid', 'all'],
        default='all',
        help="Which approach to demonstrate (default: all)"
    )

    args = parser.parse_args()
    asyncio.run(run_demo(args.approach))


if __name__ == "__main__":
    main()