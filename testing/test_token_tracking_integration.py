#!/usr/bin/env python3
"""
Test script to validate token tracking integration with error detection approaches
"""

import asyncio
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.models.error_detection_approaches import create_approach
from src.utils.api_tracker import api_tracker
from scripts.create_dataset import create_real_dataset
from PIL import Image
import requests
from io import BytesIO

async def test_approach_with_token_tracking(approach_name: str):
    """Test a specific approach with token tracking"""
    print(f"\n{'='*60}")
    print(f"TESTING {approach_name.upper().replace('_', '‚Üí')} WITH TOKEN TRACKING")
    print(f"{'='*60}")

    try:
        # Reset tracker for clean test
        api_tracker.reset_session()

        # Create approach
        approach = create_approach(approach_name)
        print(f"‚úÖ Created {approach_name} approach")

        # Get a sample from the dataset
        dataset = create_real_dataset()
        sample = dataset.samples[0]  # Use first sample

        # Convert file URLs to actual images for testing
        # For this test, we'll use placeholder images
        question_image = Image.new('RGB', (400, 300), color='white')
        solution_image = Image.new('RGB', (400, 300), color='lightgray')

        print(f"üìù Processing sample: {sample.id}")
        print(f"   Topic: {sample.metadata.get('topic', 'Unknown')}")

        # Run error detection
        result = await approach.detect_errors(question_image, solution_image, {
            'sample_id': sample.id,
            'test_mode': True
        })

        print(f"üîç Analysis completed:")
        print(f"   Has error: {result.error_analysis.has_error}")
        print(f"   Confidence: {result.error_analysis.confidence:.3f}")

        # Show token tracking results
        session_summary = api_tracker.get_session_summary()
        print(f"\nüìä Token Usage Summary:")
        print(f"   Total API calls: {session_summary['total_calls']}")
        print(f"   Total tokens: {session_summary['total_tokens']:,}")
        print(f"   Total cost: ${session_summary['total_cost']:.6f}")

        if session_summary['by_purpose']:
            print(f"\n   By API Call Type:")
            for purpose, data in session_summary['by_purpose'].items():
                print(f"     {purpose}:")
                print(f"       Calls: {data['calls']}")
                print(f"       Tokens: {data['tokens']:,}")
                print(f"       Cost: ${data['cost']:.6f}")
                print(f"       Models: {', '.join(data['models'])}")

        return True

    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """Test token tracking across all approaches"""
    print("TOKEN TRACKING INTEGRATION TEST")
    print("=" * 80)

    approaches = ["ocr_llm", "vlm_direct", "hybrid"]
    results = {}

    for approach in approaches:
        try:
            success = await test_approach_with_token_tracking(approach)
            results[approach] = success
        except Exception as e:
            print(f"‚ùå Failed to test {approach}: {e}")
            results[approach] = False

    # Summary
    print(f"\n{'='*80}")
    print("TEST SUMMARY")
    print(f"{'='*80}")

    passed = sum(results.values())
    total = len(results)

    print(f"Approaches tested: {total}")
    print(f"Successful tests: {passed}")

    for approach, success in results.items():
        status = "‚úÖ" if success else "‚ùå"
        print(f"   {status} {approach.upper().replace('_', '‚Üí')}")

    if passed == total:
        print(f"\nüéâ All token tracking integration tests passed!")
        print(f"   ‚Ä¢ API calls are being tracked correctly")
        print(f"   ‚Ä¢ Token counts are captured from OpenAI responses")
        print(f"   ‚Ä¢ Token estimates work for Gemini responses")
        print(f"   ‚Ä¢ Costs are calculated accurately using 2025 pricing")
    else:
        print(f"\n‚ö†Ô∏è  Some tests failed. Check API configuration and error messages above.")

    print(f"\nüí° Next step: Run full evaluation with `make eval` to see token tracking in action!")

if __name__ == "__main__":
    asyncio.run(main())