# Multi-Model Router: Gemini + OpenAI Support

This system now supports both **Gemini 2.5 Flash** and **OpenAI GPT-4o** with automatic provider switching and Pydantic structured outputs.

## üöÄ Key Features

‚úÖ **Unified API**: Same interface for both providers
‚úÖ **Pydantic Structured Outputs**: Both providers return structured, validated responses
‚úÖ **Automatic Provider Selection**: Auto-detect based on available API keys
‚úÖ **Parallel Processing**: Optimized latency with concurrent API calls
‚úÖ **Fallback Support**: Sequential processing if parallel fails
‚úÖ **Easy Configuration**: Switch providers via environment variables

## üìä Provider Comparison

| Feature | Gemini 2.5 Flash | OpenAI GPT-4o |
|---------|------------------|---------------|
| **Structured Outputs** | ‚úÖ JSON Schema | ‚úÖ `.parse()` method |
| **Image Processing** | ‚úÖ Native vision | ‚úÖ Base64 images |
| **Performance** | ~4-8s per request | ~3-6s per request |
| **Cost** | Very low | Low-medium |
| **Reliability** | High | Very high |

## üîß Configuration

### Environment Variables

Add to your `.env` file:

```bash
# Model Provider Selection
MODEL_PROVIDER=auto  # Options: "gemini", "openai", "auto"

# Gemini Configuration (required)
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.5-flash

# OpenAI Configuration (optional)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-2024-08-06
```

### Provider Selection Logic

- **`auto`**: Use OpenAI if `OPENAI_API_KEY` is set, otherwise Gemini
- **`gemini`**: Force Gemini 2.5 Flash
- **`openai`**: Force OpenAI GPT-4o (requires API key)

## üíª Usage Examples

### 1. Basic Usage (Auto-Detection)

```python
from src.models.error_detector import ErrorDetector

# Will auto-select provider based on configuration
detector = ErrorDetector(approach="auto")

result = await detector.detect_errors(
    question_url="path/to/question.jpg",
    solution_url="path/to/solution.jpg"
)
```

### 2. Force Specific Provider

```python
# Force Gemini
detector = ErrorDetector(approach="gemini")

# Force OpenAI
detector = ErrorDetector(approach="openai")
```

### 3. Direct Multi-Model Processor

```python
from src.models.model_router import create_processor

# Create provider-specific processor
processor = create_processor("openai")

result = await processor.analyze_images_directly(
    question_image, solution_image
)
```

## üß™ Testing

Test the multi-model router:

```bash
python test_model_router.py
```

This will test:
- ‚úÖ Gemini provider functionality
- ‚úÖ OpenAI provider (if API key available)
- ‚úÖ Auto-detection logic
- ‚úÖ ErrorDetector integration

## üìã OpenAI Structured Outputs Implementation

The OpenAI integration uses the latest **Structured Outputs** feature:

```python
# OpenAI API call with Pydantic model
completion = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[...],
    response_format=YourPydanticModel,  # üéØ Structured output
    temperature=0.1
)

# Get parsed Pydantic object
result = completion.choices[0].message.parsed
```

### Key Benefits:
- **Type Safety**: Guaranteed Pydantic object response
- **Validation**: Automatic schema validation
- **Consistency**: Same structure as Gemini responses

## üîÑ Migration Guide

### From Gemini-only to Multi-Model

**Before:**
```python
detector = ErrorDetector()  # Always Gemini
```

**After:**
```python
detector = ErrorDetector(approach="auto")  # Auto-select provider
```

### Configuration Migration

**Before:**
```bash
# Only Gemini
GEMINI_API_KEY=...
```

**After:**
```bash
# Multi-model support
MODEL_PROVIDER=auto
GEMINI_API_KEY=...
OPENAI_API_KEY=...  # Optional
```

## ‚ö° Performance Optimizations

### Parallel Processing
Both providers support parallel question/solution analysis:

1. **Question analysis** + **Solution OCR** run concurrently
2. **Text-based error analysis** runs after both complete
3. **Automatic fallback** to sequential if parallel fails

### Latency Results
- **Gemini parallel**: ~4-8s per request
- **OpenAI parallel**: ~3-6s per request
- **Sequential fallback**: ~20-30s per request

## üõ†Ô∏è Dependencies

The OpenAI library is now included in `requirements.txt`. To install all dependencies:

```bash
pip install -r requirements.txt
```

Or install OpenAI separately:

```bash
pip install openai>=1.40.0
```

Both Gemini and OpenAI dependencies are included in the requirements.

## üîç Troubleshooting

### Common Issues

**1. OpenAI Import Error**
```bash
pip install openai
```

**2. Invalid API Key**
```python
# Check your .env file
OPENAI_API_KEY=sk-...  # Must start with 'sk-'
```

**3. Model Not Found**
```python
# Use supported models
OPENAI_MODEL=gpt-4o-2024-08-06  # Supports structured outputs
```

### Debugging

Enable debug logging:
```python
import logging
logging.getLogger("src.models.model_router").setLevel(logging.DEBUG)
```

## üéØ Production Recommendations

### For Speed
```bash
MODEL_PROVIDER=openai  # Slightly faster
```

### For Cost
```bash
MODEL_PROVIDER=gemini  # More cost-effective
```

### For Reliability
```bash
MODEL_PROVIDER=auto  # Automatic fallback
```

## üîÆ Future Enhancements

- **Claude 3.5 Sonnet** support
- **Azure OpenAI** integration
- **Load balancing** across providers
- **Cost tracking** per provider
- **A/B testing** framework

---

**üöÄ Ready to use!** The system maintains full backward compatibility while adding powerful multi-model capabilities.